{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fTdjTjeTXHE"
      },
      "outputs": [],
      "source": [
        "!pip install split-folders"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6ELjkR7TdHR",
        "outputId": "8a11dc5d-56bc-495b-f546-2076953dfb14"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/MyDrive/f22-dataset\"\n",
        "# output_data = \"/content/output_data\"\n",
        "output_data = \"/content/drive/MyDrive/output_data\""
      ],
      "metadata": {
        "id": "nZYpkZBpTe8C"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute this splitfolders cell, only if you're not connected to drive."
      ],
      "metadata": {
        "id": "X7K64JNtXD_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the input data into Train Test and Validation sets"
      ],
      "metadata": {
        "id": "_eHWfMDRTk5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import splitfolders\n",
        "\n",
        "# Split with a ratio.\n",
        "# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.9, .1)`.\n",
        "splitfolders.ratio(data_path, output=\"output_data\",\n",
        "    seed=1, ratio=(.90, .10), group_prefix=None, move=False) # default values"
      ],
      "metadata": {
        "id": "AbmOTU73TlqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all libraries"
      ],
      "metadata": {
        "id": "JLiLVBu3XAxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import InceptionResNetV2, MobileNet\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.layers import Conv2D, Activation, Input, Add, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Concatenate, Lambda, add, GlobalAveragePooling2D, Convolution2D, LocallyConnected2D, ZeroPadding2D, concatenate, AveragePooling2D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam,RMSprop,SGD\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Activation, Input, Add, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Concatenate, Lambda, add, GlobalAveragePooling2D, Convolution2D, LocallyConnected2D, ZeroPadding2D, concatenate, AveragePooling2D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import Normalizer\n",
        "import pickle\n",
        "from scipy.spatial.distance import cosine, euclidean, sqeuclidean, minkowski, jaccard, jensenshannon\n"
      ],
      "metadata": {
        "id": "ityOXji4Tr5h"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pickle(path):\n",
        "  with open(path, 'rb') as f:\n",
        "      encoding_dict = pickle.load(f)\n",
        "  return encoding_dict\n",
        "\n",
        "def normalize(img):\n",
        "    mean, std = img.mean(), img.std()\n",
        "    return (img - mean) / std\n",
        "\n",
        "def scaling(x, scale):\n",
        "\treturn x * scale"
      ],
      "metadata": {
        "id": "bjYnn27LU5Zp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def InceptionResNetV2(shape = (100, 100, 3)):\n",
        "\t\n",
        "\tinputs = Input(shape=shape)\n",
        "\tx = Conv2D(32, 3, strides=2, padding='valid', use_bias=False, name= 'Conv2d_1a_3x3') (inputs)\n",
        "\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_1a_3x3_BatchNorm')(x)\n",
        "\tx = Activation('relu', name='Conv2d_1a_3x3_Activation')(x)\n",
        "\tx = Conv2D(32, 3, strides=1, padding='valid', use_bias=False, name= 'Conv2d_2a_3x3') (x)\n",
        "\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_2a_3x3_BatchNorm')(x)\n",
        "\tx = Activation('relu', name='Conv2d_2a_3x3_Activation')(x)\n",
        "\tx = Conv2D(64, 3, strides=1, padding='same', use_bias=False, name= 'Conv2d_2b_3x3') (x)\n",
        "\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_2b_3x3_BatchNorm')(x)\n",
        "\tx = Activation('relu', name='Conv2d_2b_3x3_Activation')(x)\n",
        "\tx = MaxPooling2D(3, strides=2, name='MaxPool_3a_3x3')(x)\n",
        "\tx = Conv2D(80, 1, strides=1, padding='valid', use_bias=False, name= 'Conv2d_3b_1x1') (x)\n",
        "\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_3b_1x1_BatchNorm')(x)\n",
        "\tx = Activation('relu', name='Conv2d_3b_1x1_Activation')(x)\n",
        "\tx = Conv2D(192, 3, strides=1, padding='valid', use_bias=False, name= 'Conv2d_4a_3x3') (x)\n",
        "\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_4a_3x3_BatchNorm')(x)\n",
        "\tx = Activation('relu', name='Conv2d_4a_3x3_Activation')(x)\n",
        "\tx = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name= 'Conv2d_4b_3x3') (x)\n",
        "\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_4b_3x3_BatchNorm')(x)\n",
        "\tx = Activation('relu', name='Conv2d_4b_3x3_Activation')(x)\n",
        "\t\n",
        "\t# 5x Block35 (Inception-ResNet-A block):\n",
        "\tbranch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_1_Branch_0_Conv2d_1x1') (x)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Block35_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n",
        "\tbranch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_1_Branch_1_Conv2d_0a_1x1') (x)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block35_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_1_Branch_1_Conv2d_0b_3x3') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block35_1_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n",
        "\tbranch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_1_Branch_2_Conv2d_0a_1x1') (x)\n",
        "\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n",
        "\tbranch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n",
        "\tbranch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_1_Branch_2_Conv2d_0b_3x3') (branch_2)\n",
        "\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n",
        "\tbranch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n",
        "\tbranch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_1_Branch_2_Conv2d_0c_3x3') (branch_2)\n",
        "\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n",
        "\tbranch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n",
        "\tbranches = [branch_0, branch_1, branch_2]\n",
        "\tmixed = Concatenate(axis=3, name='Block35_1_Concatenate')(branches)\n",
        "\tup = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name= 'Block35_1_Conv2d_1x1') (mixed)\n",
        "\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n",
        "\tx = add([x, up])\n",
        "\tx = Activation('relu', name='Block35_1_Activation')(x)\n",
        "\t\n",
        "\tbranch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_2_Branch_0_Conv2d_1x1') (x)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Block35_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n",
        "\tbranch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_2_Branch_1_Conv2d_0a_1x1') (x)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block35_2_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_2_Branch_1_Conv2d_0b_3x3') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block35_2_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n",
        "\tbranch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_2_Branch_2_Conv2d_0a_1x1') (x)\n",
        "\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n",
        "\tbranch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n",
        "\tbranch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_2_Branch_2_Conv2d_0b_3x3') (branch_2)\n",
        "\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n",
        "\tbranch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n",
        "\tbranch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_2_Branch_2_Conv2d_0c_3x3') (branch_2)\n",
        "\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n",
        "\tbranch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n",
        "\tbranches = [branch_0, branch_1, branch_2]\n",
        "\tmixed = Concatenate(axis=3, name='Block35_2_Concatenate')(branches)\n",
        "\tup = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name= 'Block35_2_Conv2d_1x1') (mixed)\n",
        "\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n",
        "\tx = add([x, up])\n",
        "\tx = Activation('relu', name='Block35_2_Activation')(x)\n",
        "\t\n",
        "\tbranch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_3_Branch_0_Conv2d_1x1') (x)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Block35_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n",
        "\tbranch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_3_Branch_1_Conv2d_0a_1x1') (x)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block35_3_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_3_Branch_1_Conv2d_0b_3x3') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block35_3_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n",
        "\tbranch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_3_Branch_2_Conv2d_0a_1x1') (x)\n",
        "\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n",
        "\tbranch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n",
        "\tbranch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_3_Branch_2_Conv2d_0b_3x3') (branch_2)\n",
        "\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n",
        "\tbranch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n",
        "\tbranch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_3_Branch_2_Conv2d_0c_3x3') (branch_2)\n",
        "\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n",
        "\tbranch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n",
        "\tbranches = [branch_0, branch_1, branch_2]\n",
        "\tmixed = Concatenate(axis=3, name='Block35_3_Concatenate')(branches)\n",
        "\tup = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name= 'Block35_3_Conv2d_1x1') (mixed)\n",
        "\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n",
        "\tx = add([x, up])\n",
        "\tx = Activation('relu', name='Block35_3_Activation')(x)\n",
        "\t\n",
        "\tbranch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_4_Branch_0_Conv2d_1x1') (x)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Block35_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n",
        "\tbranch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_4_Branch_1_Conv2d_0a_1x1') (x)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block35_4_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_4_Branch_1_Conv2d_0b_3x3') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block35_4_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n",
        "\tbranch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_4_Branch_2_Conv2d_0a_1x1') (x)\n",
        "\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n",
        "\tbranch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n",
        "\tbranch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_4_Branch_2_Conv2d_0b_3x3') (branch_2)\n",
        "\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n",
        "\tbranch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n",
        "\tbranch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_4_Branch_2_Conv2d_0c_3x3') (branch_2)\n",
        "\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n",
        "\tbranch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n",
        "\tbranches = [branch_0, branch_1, branch_2]\n",
        "\tmixed = Concatenate(axis=3, name='Block35_4_Concatenate')(branches)\n",
        "\tup = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name= 'Block35_4_Conv2d_1x1') (mixed)\n",
        "\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n",
        "\tx = add([x, up])\n",
        "\tx = Activation('relu', name='Block35_4_Activation')(x)\n",
        "\t\n",
        "\tbranch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_5_Branch_0_Conv2d_1x1') (x)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Block35_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n",
        "\tbranch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_5_Branch_1_Conv2d_0a_1x1') (x)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block35_5_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_5_Branch_1_Conv2d_0b_3x3') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block35_5_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n",
        "\tbranch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_5_Branch_2_Conv2d_0a_1x1') (x)\n",
        "\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n",
        "\tbranch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n",
        "\tbranch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_5_Branch_2_Conv2d_0b_3x3') (branch_2)\n",
        "\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n",
        "\tbranch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n",
        "\tbranch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_5_Branch_2_Conv2d_0c_3x3') (branch_2)\n",
        "\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n",
        "\tbranch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n",
        "\tbranches = [branch_0, branch_1, branch_2]\n",
        "\tmixed = Concatenate(axis=3, name='Block35_5_Concatenate')(branches)\n",
        "\tup = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name= 'Block35_5_Conv2d_1x1') (mixed)\n",
        "\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n",
        "\tx = add([x, up])\n",
        "\tx = Activation('relu', name='Block35_5_Activation')(x)\n",
        "\n",
        "\t# Mixed 6a (Reduction-A block):\n",
        "\tbranch_0 = Conv2D(384, 3, strides=2, padding='valid', use_bias=False, name= 'Mixed_6a_Branch_0_Conv2d_1a_3x3') (x)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_0_Conv2d_1a_3x3_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Mixed_6a_Branch_0_Conv2d_1a_3x3_Activation')(branch_0)\n",
        "\tbranch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Mixed_6a_Branch_1_Conv2d_0a_1x1') (x)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(192, 3, strides=1, padding='same', use_bias=False, name= 'Mixed_6a_Branch_1_Conv2d_0b_3x3') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name= 'Mixed_6a_Branch_1_Conv2d_1a_3x3') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_1a_3x3_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_1a_3x3_Activation')(branch_1)\n",
        "\tbranch_pool = MaxPooling2D(3, strides=2, padding='valid', name='Mixed_6a_Branch_2_MaxPool_1a_3x3')(x)\n",
        "\tbranches = [branch_0, branch_1, branch_pool]\n",
        "\tx = Concatenate(axis=3, name='Mixed_6a')(branches)\n",
        "\n",
        "\t# 10x Block17 (Inception-ResNet-B block):\n",
        "\tbranch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_1_Branch_0_Conv2d_1x1') (x)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Block17_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n",
        "\tbranch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_1_Branch_1_Conv2d_0a_1x1') (x)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name= 'Block17_1_Branch_1_Conv2d_0b_1x7') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0b_1x7_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0b_1x7_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name= 'Block17_1_Branch_1_Conv2d_0c_7x1') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0c_7x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0c_7x1_Activation')(branch_1)\n",
        "\tbranches = [branch_0, branch_1]\n",
        "\tmixed = Concatenate(axis=3, name='Block17_1_Concatenate')(branches)\n",
        "\tup = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name= 'Block17_1_Conv2d_1x1') (mixed)\n",
        "\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n",
        "\tx = add([x, up])\n",
        "\tx = Activation('relu', name='Block17_1_Activation')(x)\n",
        "\t\n",
        "\tbranch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_2_Branch_0_Conv2d_1x1') (x)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Block17_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n",
        "\tbranch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_2_Branch_2_Conv2d_0a_1x1') (x)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name= 'Block17_2_Branch_2_Conv2d_0b_1x7') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0b_1x7_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0b_1x7_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name= 'Block17_2_Branch_2_Conv2d_0c_7x1') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0c_7x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0c_7x1_Activation')(branch_1)\n",
        "\tbranches = [branch_0, branch_1]\n",
        "\tmixed = Concatenate(axis=3, name='Block17_2_Concatenate')(branches)\n",
        "\tup = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name= 'Block17_2_Conv2d_1x1') (mixed)\n",
        "\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n",
        "\tx = add([x, up])\n",
        "\tx = Activation('relu', name='Block17_2_Activation')(x)\n",
        "\t\n",
        "\tbranch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_3_Branch_0_Conv2d_1x1') (x)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Block17_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n",
        "\tbranch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_3_Branch_3_Conv2d_0a_1x1') (x)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0a_1x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0a_1x1_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name= 'Block17_3_Branch_3_Conv2d_0b_1x7') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0b_1x7_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0b_1x7_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name= 'Block17_3_Branch_3_Conv2d_0c_7x1') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0c_7x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0c_7x1_Activation')(branch_1)\n",
        "\tbranches = [branch_0, branch_1]\n",
        "\tmixed = Concatenate(axis=3, name='Block17_3_Concatenate')(branches)\n",
        "\tup = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name= 'Block17_3_Conv2d_1x1') (mixed)\n",
        "\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n",
        "\tx = add([x, up])\n",
        "\tx = Activation('relu', name='Block17_3_Activation')(x)\n",
        "\t\n",
        "\tbranch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_4_Branch_0_Conv2d_1x1') (x)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Block17_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n",
        "\tbranch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_4_Branch_4_Conv2d_0a_1x1') (x)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0a_1x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0a_1x1_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name= 'Block17_4_Branch_4_Conv2d_0b_1x7') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0b_1x7_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0b_1x7_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name= 'Block17_4_Branch_4_Conv2d_0c_7x1') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0c_7x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0c_7x1_Activation')(branch_1)\n",
        "\tbranches = [branch_0, branch_1]\n",
        "\tmixed = Concatenate(axis=3, name='Block17_4_Concatenate')(branches)\n",
        "\tup = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name= 'Block17_4_Conv2d_1x1') (mixed)\n",
        "\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n",
        "\tx = add([x, up])\n",
        "\tx = Activation('relu', name='Block17_4_Activation')(x)\n",
        "\t\n",
        "\tbranch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_5_Branch_0_Conv2d_1x1') (x)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Block17_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n",
        "\tbranch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_5_Branch_5_Conv2d_0a_1x1') (x)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0a_1x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0a_1x1_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name= 'Block17_5_Branch_5_Conv2d_0b_1x7') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0b_1x7_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0b_1x7_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name= 'Block17_5_Branch_5_Conv2d_0c_7x1') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0c_7x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0c_7x1_Activation')(branch_1)\n",
        "\tbranches = [branch_0, branch_1]\n",
        "\tmixed = Concatenate(axis=3, name='Block17_5_Concatenate')(branches)\n",
        "\tup = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name= 'Block17_5_Conv2d_1x1') (mixed)\n",
        "\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n",
        "\tx = add([x, up])\n",
        "\tx = Activation('relu', name='Block17_5_Activation')(x)\n",
        "\t\n",
        "\tbranch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_6_Branch_0_Conv2d_1x1') (x)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Block17_6_Branch_0_Conv2d_1x1_Activation')(branch_0)\n",
        "\tbranch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_6_Branch_6_Conv2d_0a_1x1') (x)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0a_1x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0a_1x1_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name= 'Block17_6_Branch_6_Conv2d_0b_1x7') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0b_1x7_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0b_1x7_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name= 'Block17_6_Branch_6_Conv2d_0c_7x1') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0c_7x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0c_7x1_Activation')(branch_1)\n",
        "\tbranches = [branch_0, branch_1]\n",
        "\tmixed = Concatenate(axis=3, name='Block17_6_Concatenate')(branches)\n",
        "\tup = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name= 'Block17_6_Conv2d_1x1') (mixed)\n",
        "\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n",
        "\tx = add([x, up])\n",
        "\tx = Activation('relu', name='Block17_6_Activation')(x)\t\n",
        "\t\n",
        "\tbranch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_7_Branch_0_Conv2d_1x1') (x)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Block17_7_Branch_0_Conv2d_1x1_Activation')(branch_0)\n",
        "\tbranch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_7_Branch_7_Conv2d_0a_1x1') (x)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0a_1x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0a_1x1_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name= 'Block17_7_Branch_7_Conv2d_0b_1x7') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0b_1x7_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0b_1x7_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name= 'Block17_7_Branch_7_Conv2d_0c_7x1') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0c_7x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0c_7x1_Activation')(branch_1)\n",
        "\tbranches = [branch_0, branch_1]\n",
        "\tmixed = Concatenate(axis=3, name='Block17_7_Concatenate')(branches)\n",
        "\tup = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name= 'Block17_7_Conv2d_1x1') (mixed)\n",
        "\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n",
        "\tx = add([x, up])\n",
        "\tx = Activation('relu', name='Block17_7_Activation')(x)\n",
        "\t\n",
        "\tbranch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_8_Branch_0_Conv2d_1x1') (x)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Block17_8_Branch_0_Conv2d_1x1_Activation')(branch_0)\n",
        "\tbranch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_8_Branch_8_Conv2d_0a_1x1') (x)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0a_1x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0a_1x1_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name= 'Block17_8_Branch_8_Conv2d_0b_1x7') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0b_1x7_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0b_1x7_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name= 'Block17_8_Branch_8_Conv2d_0c_7x1') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0c_7x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0c_7x1_Activation')(branch_1)\n",
        "\tbranches = [branch_0, branch_1]\n",
        "\tmixed = Concatenate(axis=3, name='Block17_8_Concatenate')(branches)\n",
        "\tup = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name= 'Block17_8_Conv2d_1x1') (mixed)\n",
        "\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n",
        "\tx = add([x, up])\n",
        "\tx = Activation('relu', name='Block17_8_Activation')(x)\n",
        "\t\n",
        "\tbranch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_9_Branch_0_Conv2d_1x1') (x)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Block17_9_Branch_0_Conv2d_1x1_Activation')(branch_0)\n",
        "\tbranch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_9_Branch_9_Conv2d_0a_1x1') (x)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0a_1x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0a_1x1_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name= 'Block17_9_Branch_9_Conv2d_0b_1x7') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0b_1x7_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0b_1x7_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name= 'Block17_9_Branch_9_Conv2d_0c_7x1') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0c_7x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0c_7x1_Activation')(branch_1)\n",
        "\tbranches = [branch_0, branch_1]\n",
        "\tmixed = Concatenate(axis=3, name='Block17_9_Concatenate')(branches)\n",
        "\tup = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name= 'Block17_9_Conv2d_1x1') (mixed)\n",
        "\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n",
        "\tx = add([x, up])\n",
        "\tx = Activation('relu', name='Block17_9_Activation')(x)\n",
        "\t\n",
        "\tbranch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_10_Branch_0_Conv2d_1x1') (x)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Block17_10_Branch_0_Conv2d_1x1_Activation')(branch_0)\n",
        "\tbranch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_10_Branch_10_Conv2d_0a_1x1') (x)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0a_1x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0a_1x1_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name= 'Block17_10_Branch_10_Conv2d_0b_1x7') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0b_1x7_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0b_1x7_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name= 'Block17_10_Branch_10_Conv2d_0c_7x1') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0c_7x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0c_7x1_Activation')(branch_1)\n",
        "\tbranches = [branch_0, branch_1]\n",
        "\tmixed = Concatenate(axis=3, name='Block17_10_Concatenate')(branches)\n",
        "\tup = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name= 'Block17_10_Conv2d_1x1') (mixed)\n",
        "\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n",
        "\tx = add([x, up])\n",
        "\tx = Activation('relu', name='Block17_10_Activation')(x)\n",
        "\n",
        "\t# Mixed 7a (Reduction-B block): 8 x 8 x 2080\t\n",
        "\tbranch_0 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name= 'Mixed_7a_Branch_0_Conv2d_0a_1x1') (x)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_0_Conv2d_0a_1x1_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Mixed_7a_Branch_0_Conv2d_0a_1x1_Activation')(branch_0)\n",
        "\tbranch_0 = Conv2D(384, 3, strides=2, padding='valid', use_bias=False, name= 'Mixed_7a_Branch_0_Conv2d_1a_3x3') (branch_0)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_0_Conv2d_1a_3x3_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Mixed_7a_Branch_0_Conv2d_1a_3x3_Activation')(branch_0)\n",
        "\tbranch_1 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name= 'Mixed_7a_Branch_1_Conv2d_0a_1x1') (x)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Mixed_7a_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name= 'Mixed_7a_Branch_1_Conv2d_1a_3x3') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_1_Conv2d_1a_3x3_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Mixed_7a_Branch_1_Conv2d_1a_3x3_Activation')(branch_1)\n",
        "\tbranch_2 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name= 'Mixed_7a_Branch_2_Conv2d_0a_1x1') (x)\n",
        "\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n",
        "\tbranch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n",
        "\tbranch_2 = Conv2D(256, 3, strides=1, padding='same', use_bias=False, name= 'Mixed_7a_Branch_2_Conv2d_0b_3x3') (branch_2)\n",
        "\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n",
        "\tbranch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n",
        "\tbranch_2 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name= 'Mixed_7a_Branch_2_Conv2d_1a_3x3') (branch_2)\n",
        "\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_1a_3x3_BatchNorm')(branch_2)\n",
        "\tbranch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_1a_3x3_Activation')(branch_2)\n",
        "\tbranch_pool = MaxPooling2D(3, strides=2, padding='valid', name='Mixed_7a_Branch_3_MaxPool_1a_3x3')(x)\n",
        "\tbranches = [branch_0, branch_1, branch_2, branch_pool]\n",
        "\tx = Concatenate(axis=3, name='Mixed_7a')(branches)\n",
        "\n",
        "\t# 5x Block8 (Inception-ResNet-C block):\n",
        "\t\n",
        "\tbranch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_1_Branch_0_Conv2d_1x1') (x)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Block8_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n",
        "\tbranch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_1_Branch_1_Conv2d_0a_1x1') (x)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name= 'Block8_1_Branch_1_Conv2d_0b_1x3') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0b_1x3_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0b_1x3_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name= 'Block8_1_Branch_1_Conv2d_0c_3x1') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0c_3x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0c_3x1_Activation')(branch_1)\n",
        "\tbranches = [branch_0, branch_1]\n",
        "\tmixed = Concatenate(axis=3, name='Block8_1_Concatenate')(branches)\n",
        "\tup = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name= 'Block8_1_Conv2d_1x1') (mixed)\n",
        "\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n",
        "\tx = add([x, up])\n",
        "\tx = Activation('relu', name='Block8_1_Activation')(x)\n",
        "\t\n",
        "\tbranch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_2_Branch_0_Conv2d_1x1') (x)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Block8_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n",
        "\tbranch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_2_Branch_2_Conv2d_0a_1x1') (x)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name= 'Block8_2_Branch_2_Conv2d_0b_1x3') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0b_1x3_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0b_1x3_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name= 'Block8_2_Branch_2_Conv2d_0c_3x1') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0c_3x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0c_3x1_Activation')(branch_1)\n",
        "\tbranches = [branch_0, branch_1]\n",
        "\tmixed = Concatenate(axis=3, name='Block8_2_Concatenate')(branches)\n",
        "\tup = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name= 'Block8_2_Conv2d_1x1') (mixed)\n",
        "\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n",
        "\tx = add([x, up])\n",
        "\tx = Activation('relu', name='Block8_2_Activation')(x)\n",
        "\t\n",
        "\tbranch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_3_Branch_0_Conv2d_1x1') (x)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Block8_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n",
        "\tbranch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_3_Branch_3_Conv2d_0a_1x1') (x)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0a_1x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0a_1x1_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name= 'Block8_3_Branch_3_Conv2d_0b_1x3') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0b_1x3_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0b_1x3_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name= 'Block8_3_Branch_3_Conv2d_0c_3x1') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0c_3x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0c_3x1_Activation')(branch_1)\n",
        "\tbranches = [branch_0, branch_1]\n",
        "\tmixed = Concatenate(axis=3, name='Block8_3_Concatenate')(branches)\n",
        "\tup = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name= 'Block8_3_Conv2d_1x1') (mixed)\n",
        "\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n",
        "\tx = add([x, up])\n",
        "\tx = Activation('relu', name='Block8_3_Activation')(x)\n",
        "\t\n",
        "\tbranch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_4_Branch_0_Conv2d_1x1') (x)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Block8_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n",
        "\tbranch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_4_Branch_4_Conv2d_0a_1x1') (x)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0a_1x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0a_1x1_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name= 'Block8_4_Branch_4_Conv2d_0b_1x3') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0b_1x3_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0b_1x3_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name= 'Block8_4_Branch_4_Conv2d_0c_3x1') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0c_3x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0c_3x1_Activation')(branch_1)\n",
        "\tbranches = [branch_0, branch_1]\n",
        "\tmixed = Concatenate(axis=3, name='Block8_4_Concatenate')(branches)\n",
        "\tup = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name= 'Block8_4_Conv2d_1x1') (mixed)\n",
        "\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n",
        "\tx = add([x, up])\n",
        "\tx = Activation('relu', name='Block8_4_Activation')(x)\n",
        "\t\n",
        "\tbranch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_5_Branch_0_Conv2d_1x1') (x)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Block8_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n",
        "\tbranch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_5_Branch_5_Conv2d_0a_1x1') (x)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0a_1x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0a_1x1_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name= 'Block8_5_Branch_5_Conv2d_0b_1x3') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0b_1x3_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0b_1x3_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name= 'Block8_5_Branch_5_Conv2d_0c_3x1') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0c_3x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0c_3x1_Activation')(branch_1)\n",
        "\tbranches = [branch_0, branch_1]\n",
        "\tmixed = Concatenate(axis=3, name='Block8_5_Concatenate')(branches)\n",
        "\tup = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name= 'Block8_5_Conv2d_1x1') (mixed)\n",
        "\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n",
        "\tx = add([x, up])\n",
        "\tx = Activation('relu', name='Block8_5_Activation')(x)\n",
        "\t\n",
        "\tbranch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_6_Branch_0_Conv2d_1x1') (x)\n",
        "\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n",
        "\tbranch_0 = Activation('relu', name='Block8_6_Branch_0_Conv2d_1x1_Activation')(branch_0)\n",
        "\tbranch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_6_Branch_1_Conv2d_0a_1x1') (x)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name= 'Block8_6_Branch_1_Conv2d_0b_1x3') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0b_1x3_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0b_1x3_Activation')(branch_1)\n",
        "\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name= 'Block8_6_Branch_1_Conv2d_0c_3x1') (branch_1)\n",
        "\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0c_3x1_BatchNorm')(branch_1)\n",
        "\tbranch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0c_3x1_Activation')(branch_1)\n",
        "\tbranches = [branch_0, branch_1]\n",
        "\tmixed = Concatenate(axis=3, name='Block8_6_Concatenate')(branches)\n",
        "\tup = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name= 'Block8_6_Conv2d_1x1') (mixed)\n",
        "\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 1})(up)\n",
        "\tx = add([x, up])\n",
        "\t\n",
        "\t# Classification block\n",
        "\tx = GlobalAveragePooling2D(name='AvgPool')(x)\n",
        "\tx = Dropout(1.0 - 0.8, name='Dropout')(x)\n",
        "\t# Bottleneck\n",
        "\tx = Dense(128, use_bias=False, name='Bottleneck')(x)\n",
        "\tx = BatchNormalization(momentum=0.995, epsilon=0.001, scale=False, name='Bottleneck_BatchNorm')(x)\n",
        "\n",
        "\t# Create model\n",
        "\tmodel = Model(inputs, x, name='inception_resnet_v2')\n",
        "\n",
        "\treturn model\n",
        "\n",
        "# Create the FaceNet model\n",
        "face_encoder = InceptionResNetV2()\n",
        "\n",
        "# # Load the weights of the model\n",
        "path = \"/content/drive/MyDrive/IR_Weights/facenet_keras_weights.h5\"\n",
        "face_encoder.load_weights(path)"
      ],
      "metadata": {
        "id": "HmoKqsccUGN0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######pathsandvairables#########\n",
        "face_data = output_data + '/train'\n",
        "test_data = output_data + '/val'\n",
        "required_shape = (100,100)\n",
        "encodes = []\n",
        "encoding_dict = dict()\n",
        "l2_normalizer = Normalizer('l2')\n",
        "recognition_t=0.5\n",
        "distance = float(\"inf\")\n",
        "###############################"
      ],
      "metadata": {
        "id": "hZPzd6xOVBUg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "zHE3nASHVfjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for face_names in os.listdir(face_data):\n",
        "    person_dir = os.path.join(face_data,face_names)\n",
        "\n",
        "    for image_name in os.listdir(person_dir):\n",
        "        image_path = os.path.join(person_dir,image_name)\n",
        "\n",
        "        img_BGR = cv.imread(image_path)\n",
        "        img_RGB = cv.cvtColor(img_BGR, cv.COLOR_BGR2RGB)\n",
        "        \n",
        "        face = normalize(img_RGB)\n",
        "        face = cv.resize(face, required_shape, interpolation = cv.INTER_CUBIC)\n",
        "\n",
        "        face_d = np.expand_dims(face, axis=0)\n",
        "\n",
        "        encode = face_encoder.predict(face_d)[0]\n",
        "        encodes.append(encode)\n",
        "\n",
        "    if encodes:\n",
        "        encode = np.sum(encodes, axis=0)/len(encodes)\n",
        "        encode = l2_normalizer.fit_transform(np.expand_dims(encode, axis=0))[0]\n",
        "        encoding_dict[face_names] = encode\n",
        "\n",
        "    encodes =[]"
      ],
      "metadata": {
        "id": "dXCyeneQVgU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save The enodings and copy them to drive"
      ],
      "metadata": {
        "id": "_6bjba-0VpsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('/content/encodings/'):\n",
        "  os.makedirs('/content/encodings')\n",
        "path = '/content/encodings/normalized_encodings.pkl'\n",
        "with open(path, 'wb') as file:\n",
        "    pickle.dump(encoding_dict, file)\n",
        "\n",
        "!cp -r '/content/encodings' '/content/drive/MyDrive'"
      ],
      "metadata": {
        "id": "8kP2i0j4Vlnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the encoding dict from drive."
      ],
      "metadata": {
        "id": "rJOKGC53Vvsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding_dict = load_pickle(\"/content/drive/MyDrive/encodings/encodings.pkl\")"
      ],
      "metadata": {
        "id": "h86Hdh4PVx79"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fusion Testing"
      ],
      "metadata": {
        "id": "-e9lt8hvWCLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TP=0\n",
        "TOTAL=0\n",
        "folder=0\n",
        "distance = float(\"inf\")\n",
        "research = []\n",
        "Actual = []\n",
        "Predicted = []\n",
        "for face_names in os.listdir(test_data):\n",
        "    person_dir = os.path.join(test_data,face_names)\n",
        "\n",
        "    for image_name in os.listdir(person_dir):\n",
        "        image_path = os.path.join(person_dir,image_name)\n",
        "\n",
        "        img_BGR = cv.imread(image_path)\n",
        "        img_RGB = cv.cvtColor(img_BGR, cv.COLOR_BGR2RGB)\n",
        "        \n",
        "        face = normalize(img_RGB)\n",
        "        face = cv.resize(face, required_shape, interpolation = cv.INTER_CUBIC)\n",
        "\n",
        "        face_d = np.expand_dims(face, axis=0)\n",
        "\n",
        "        test_encode = face_encoder.predict(face_d)[0]\n",
        "\n",
        "        for db_name, db_encode in encoding_dict.items():\n",
        "          dist = cosine(db_encode, test_encode)\n",
        "          dist_0 = euclidean(db_encode, test_encode)\n",
        "          dist_f = 0.3 * dist_0 + 0.7 * dist\n",
        "          \n",
        "          if dist < recognition_t and dist < distance:\n",
        "              name = db_name\n",
        "              distance = dist\n",
        "        distance = float(\"inf\")\n",
        "        Actual.append(face_names)\n",
        "        Predicted.append(name)\n",
        "        if name == face_names:\n",
        "          TP+=1\n",
        "        else:\n",
        "          print(\"predicted = \",name,\" -> but expected =\", face_names)\n",
        "          research.append([face_names, image_name])\n",
        "        TOTAL+=1\n",
        "        print(\"True Positives = \", TP,\"Total_images =\", TOTAL)\n",
        "    \n",
        "print(\"Accuracy = \", TP/TOTAL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH1aB5WTV1co",
        "outputId": "320c3e97-249c-4aaa-ced7-56ca8b5abe40"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 9s 9s/step\n",
            "True Positives =  1 Total_images = 1\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  2 Total_images = 2\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  3 Total_images = 3\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "True Positives =  4 Total_images = 4\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  5 Total_images = 5\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "predicted =  27  -> but expected = 28\n",
            "True Positives =  5 Total_images = 6\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "predicted =  50  -> but expected = 28\n",
            "True Positives =  5 Total_images = 7\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "True Positives =  6 Total_images = 8\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  7 Total_images = 9\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  8 Total_images = 10\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  9 Total_images = 11\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  10 Total_images = 12\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  11 Total_images = 13\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  12 Total_images = 14\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  13 Total_images = 15\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  14 Total_images = 16\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "predicted =  11  -> but expected = 21\n",
            "True Positives =  14 Total_images = 17\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  15 Total_images = 18\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  16 Total_images = 19\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  17 Total_images = 20\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  18 Total_images = 21\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  19 Total_images = 22\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "True Positives =  20 Total_images = 23\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  21 Total_images = 24\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  22 Total_images = 25\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "True Positives =  23 Total_images = 26\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  24 Total_images = 27\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  25 Total_images = 28\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  26 Total_images = 29\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "True Positives =  27 Total_images = 30\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "True Positives =  28 Total_images = 31\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  29 Total_images = 32\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  30 Total_images = 33\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "True Positives =  31 Total_images = 34\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "predicted =  27  -> but expected = 23\n",
            "True Positives =  31 Total_images = 35\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  32 Total_images = 36\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "predicted =  23  -> but expected = 24\n",
            "True Positives =  32 Total_images = 37\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "predicted =  23  -> but expected = 24\n",
            "True Positives =  32 Total_images = 38\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "predicted =  23  -> but expected = 24\n",
            "True Positives =  32 Total_images = 39\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "predicted =  23  -> but expected = 24\n",
            "True Positives =  32 Total_images = 40\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "predicted =  23  -> but expected = 20\n",
            "True Positives =  32 Total_images = 41\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  33 Total_images = 42\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "predicted =  53  -> but expected = 20\n",
            "True Positives =  33 Total_images = 43\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  34 Total_images = 44\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  35 Total_images = 45\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  36 Total_images = 46\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  37 Total_images = 47\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  38 Total_images = 48\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  39 Total_images = 49\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  40 Total_images = 50\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "True Positives =  41 Total_images = 51\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  42 Total_images = 52\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  43 Total_images = 53\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  44 Total_images = 54\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  45 Total_images = 55\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  46 Total_images = 56\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  47 Total_images = 57\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  48 Total_images = 58\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  49 Total_images = 59\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  50 Total_images = 60\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  51 Total_images = 61\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  52 Total_images = 62\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  53 Total_images = 63\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  54 Total_images = 64\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  55 Total_images = 65\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  56 Total_images = 66\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  57 Total_images = 67\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  58 Total_images = 68\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  59 Total_images = 69\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "True Positives =  60 Total_images = 70\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  61 Total_images = 71\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "True Positives =  62 Total_images = 72\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  63 Total_images = 73\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  64 Total_images = 74\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  65 Total_images = 75\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  66 Total_images = 76\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "predicted =  23  -> but expected = 15\n",
            "True Positives =  66 Total_images = 77\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  67 Total_images = 78\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  68 Total_images = 79\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  69 Total_images = 80\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "True Positives =  70 Total_images = 81\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  71 Total_images = 82\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  72 Total_images = 83\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  73 Total_images = 84\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  74 Total_images = 85\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  75 Total_images = 86\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  76 Total_images = 87\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "predicted =  33  -> but expected = 11\n",
            "True Positives =  76 Total_images = 88\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  77 Total_images = 89\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  78 Total_images = 90\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "True Positives =  79 Total_images = 91\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  80 Total_images = 92\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  81 Total_images = 93\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  82 Total_images = 94\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "True Positives =  83 Total_images = 95\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "True Positives =  84 Total_images = 96\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  85 Total_images = 97\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  86 Total_images = 98\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  87 Total_images = 99\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  88 Total_images = 100\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  89 Total_images = 101\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  90 Total_images = 102\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  91 Total_images = 103\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "True Positives =  92 Total_images = 104\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "True Positives =  93 Total_images = 105\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  94 Total_images = 106\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  95 Total_images = 107\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  96 Total_images = 108\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "True Positives =  97 Total_images = 109\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  98 Total_images = 110\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  99 Total_images = 111\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  100 Total_images = 112\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "True Positives =  101 Total_images = 113\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "predicted =  27  -> but expected = 05\n",
            "True Positives =  101 Total_images = 114\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  102 Total_images = 115\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  103 Total_images = 116\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "predicted =  27  -> but expected = 50\n",
            "True Positives =  103 Total_images = 117\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  104 Total_images = 118\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "True Positives =  105 Total_images = 119\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  106 Total_images = 120\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  107 Total_images = 121\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  108 Total_images = 122\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "predicted =  42  -> but expected = 54\n",
            "True Positives =  108 Total_images = 123\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  109 Total_images = 124\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  110 Total_images = 125\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "predicted =  42  -> but expected = 53\n",
            "True Positives =  110 Total_images = 126\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  111 Total_images = 127\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  112 Total_images = 128\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  113 Total_images = 129\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "predicted =  10  -> but expected = 49\n",
            "True Positives =  113 Total_images = 130\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "predicted =  50  -> but expected = 49\n",
            "True Positives =  113 Total_images = 131\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  114 Total_images = 132\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  115 Total_images = 133\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  116 Total_images = 134\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "predicted =  27  -> but expected = 48\n",
            "True Positives =  116 Total_images = 135\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "predicted =  50  -> but expected = 48\n",
            "True Positives =  116 Total_images = 136\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  117 Total_images = 137\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "predicted =  27  -> but expected = 51\n",
            "True Positives =  117 Total_images = 138\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "predicted =  27  -> but expected = 51\n",
            "True Positives =  117 Total_images = 139\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  118 Total_images = 140\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  119 Total_images = 141\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  120 Total_images = 142\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  121 Total_images = 143\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  122 Total_images = 144\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  123 Total_images = 145\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  124 Total_images = 146\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  125 Total_images = 147\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  126 Total_images = 148\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  127 Total_images = 149\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  128 Total_images = 150\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  129 Total_images = 151\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  130 Total_images = 152\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "predicted =  12  -> but expected = 56\n",
            "True Positives =  130 Total_images = 153\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  131 Total_images = 154\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  132 Total_images = 155\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  133 Total_images = 156\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  134 Total_images = 157\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "predicted =  10  -> but expected = 42\n",
            "True Positives =  134 Total_images = 158\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  135 Total_images = 159\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "predicted =  10  -> but expected = 42\n",
            "True Positives =  135 Total_images = 160\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "predicted =  10  -> but expected = 42\n",
            "True Positives =  135 Total_images = 161\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  136 Total_images = 162\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  137 Total_images = 163\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "True Positives =  138 Total_images = 164\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  139 Total_images = 165\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "True Positives =  140 Total_images = 166\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  141 Total_images = 167\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  142 Total_images = 168\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  143 Total_images = 169\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  144 Total_images = 170\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  145 Total_images = 171\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  146 Total_images = 172\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  147 Total_images = 173\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "predicted =  25  -> but expected = 41\n",
            "True Positives =  147 Total_images = 174\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  148 Total_images = 175\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  149 Total_images = 176\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  150 Total_images = 177\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  151 Total_images = 178\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  152 Total_images = 179\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  153 Total_images = 180\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  154 Total_images = 181\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "predicted =  50  -> but expected = 43\n",
            "True Positives =  154 Total_images = 182\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "predicted =  27  -> but expected = 43\n",
            "True Positives =  154 Total_images = 183\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  155 Total_images = 184\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  156 Total_images = 185\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  157 Total_images = 186\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  158 Total_images = 187\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  159 Total_images = 188\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  160 Total_images = 189\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  161 Total_images = 190\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  162 Total_images = 191\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  163 Total_images = 192\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  164 Total_images = 193\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "predicted =  23  -> but expected = 40\n",
            "True Positives =  164 Total_images = 194\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  165 Total_images = 195\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  166 Total_images = 196\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "predicted =  05  -> but expected = 40\n",
            "True Positives =  166 Total_images = 197\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  167 Total_images = 198\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  168 Total_images = 199\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  169 Total_images = 200\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "True Positives =  170 Total_images = 201\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  171 Total_images = 202\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  172 Total_images = 203\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  173 Total_images = 204\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  174 Total_images = 205\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "predicted =  50  -> but expected = 36\n",
            "True Positives =  174 Total_images = 206\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "predicted =  51  -> but expected = 36\n",
            "True Positives =  174 Total_images = 207\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  175 Total_images = 208\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  176 Total_images = 209\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  177 Total_images = 210\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "True Positives =  178 Total_images = 211\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  179 Total_images = 212\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "predicted =  10  -> but expected = 30\n",
            "True Positives =  179 Total_images = 213\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  180 Total_images = 214\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  181 Total_images = 215\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  182 Total_images = 216\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  183 Total_images = 217\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "True Positives =  184 Total_images = 218\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  185 Total_images = 219\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "predicted =  54  -> but expected = 33\n",
            "True Positives =  185 Total_images = 220\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "predicted =  50  -> but expected = 33\n",
            "True Positives =  185 Total_images = 221\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  186 Total_images = 222\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  187 Total_images = 223\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  188 Total_images = 224\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "True Positives =  189 Total_images = 225\n",
            "Accuracy =  0.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "Actual = [int(i) for i in Actual]\n",
        "Predicted = [int(i) for i in Predicted]\n",
        "confusion_matrix = metrics.confusion_matrix(Actual, Predicted)\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix, display_labels = None)\n",
        "fig, ax = plt.subplots(figsize=(25,25))\n",
        "cm_display.plot(ax = ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Pv5dhAY8HUmL",
        "outputId": "1017f863-672f-45cc-d0e9-a0f4f98ebb8a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1800x1800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)  \n",
        "FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
        "TP = np.diag(confusion_matrix)\n",
        "TN = confusion_matrix.sum() - (FP + FN + TP)\n",
        "\n",
        "# Sensitivity, hit rate, recall, or true positive rate\n",
        "TPR = TP/(TP+FN)\n",
        "# Specificity or true negative rate\n",
        "TNR = TN/(TN+FP) \n",
        "# Precision or positive predictive value\n",
        "PPV = TP/(TP+FP)\n",
        "# Negative predictive value\n",
        "NPV = TN/(TN+FN)\n",
        "# Fall out or false positive rate\n",
        "FPR = FP/(FP+TN)\n",
        "# False negative rate\n",
        "FNR = FN/(TP+FN)\n",
        "# False discovery rate\n",
        "FDR = FP/(TP+FP)\n",
        "\n",
        "# Overall accuracy\n",
        "ACC = (TP+TN)/(TP+FP+FN+TN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnsXeI38T-I0",
        "outputId": "9a475e27-7100-45be-8e67-34a7fb0e9529"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in true_divide\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "d = {\"FP\": FP,\"FN\": FN,\"TP\": TP,\"TN\": TN, \"TPR\":TPR,\"TNR\": TNR,\"PPV\": PPV,\"NPV\": NPV,\"FPR\": FPR,\n",
        "\"FNR\": FNR,\n",
        "\"FDR\": FDR,\n",
        "\"ACC\": ACC}\n",
        "df = pd.DataFrame(d)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PfaK428wZDQ8",
        "outputId": "129cd255-3d69-496d-dc5b-d647b8ff2f7e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    FP  FN  TP   TN   TPR       TNR       PPV       NPV       FPR   FNR  \\\n",
              "0    0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "1    0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "2    0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "3    1   1   3  220  0.75  0.995475  0.750000  0.995475  0.004525  0.25   \n",
              "4    0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "5    0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "6    0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "7    0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "8    5   0   4  216  1.00  0.977376  0.444444  1.000000  0.022624  0.00   \n",
              "9    1   1   3  220  0.75  0.995475  0.750000  0.995475  0.004525  0.25   \n",
              "10   1   0   4  220  1.00  0.995475  0.800000  1.000000  0.004525  0.00   \n",
              "11   0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "12   0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "13   0   1   3  221  0.75  1.000000  1.000000  0.995495  0.000000  0.25   \n",
              "14   0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "15   0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "16   0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "17   0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "18   0   2   2  221  0.50  1.000000  1.000000  0.991031  0.000000  0.50   \n",
              "19   0   1   3  221  0.75  1.000000  1.000000  0.995495  0.000000  0.25   \n",
              "20   0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "21   7   1   3  214  0.75  0.968326  0.300000  0.995349  0.031674  0.25   \n",
              "22   0   4   0  221  0.00  1.000000       NaN  0.982222  0.000000  1.00   \n",
              "23   1   0   4  220  1.00  0.995475  0.800000  1.000000  0.004525  0.00   \n",
              "24   0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "25   8   0   4  213  1.00  0.963801  0.333333  1.000000  0.036199  0.00   \n",
              "26   0   2   2  221  0.50  1.000000  1.000000  0.991031  0.000000  0.50   \n",
              "27   0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "28   0   1   3  221  0.75  1.000000  1.000000  0.995495  0.000000  0.25   \n",
              "29   0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "30   0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "31   1   2   2  220  0.50  0.995475  0.666667  0.990991  0.004525  0.50   \n",
              "32   0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "33   0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "34   0   2   2  221  0.50  1.000000  1.000000  0.991031  0.000000  0.50   \n",
              "35   0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "36   0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "37   0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "38   0   2   2  221  0.50  1.000000  1.000000  0.991031  0.000000  0.50   \n",
              "39   0   1   3  221  0.75  1.000000  1.000000  0.995495  0.000000  0.25   \n",
              "40   2   3   1  219  0.25  0.990950  0.333333  0.986486  0.009050  0.75   \n",
              "41   0   2   2  221  0.50  1.000000  1.000000  0.991031  0.000000  0.50   \n",
              "42   0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "43   0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "44   0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "45   0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "46   0   2   2  221  0.50  1.000000  1.000000  0.991031  0.000000  0.50   \n",
              "47   0   2   2  221  0.50  1.000000  1.000000  0.991031  0.000000  0.50   \n",
              "48   6   1   4  214  0.80  0.972727  0.400000  0.995349  0.027273  0.20   \n",
              "49   1   2   2  220  0.50  0.995475  0.666667  0.990991  0.004525  0.50   \n",
              "50   0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "51   1   1   3  220  0.75  0.995475  0.750000  0.995475  0.004525  0.25   \n",
              "52   1   1   3  220  0.75  0.995475  0.750000  0.995475  0.004525  0.25   \n",
              "53   0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "54   0   1   3  221  0.75  1.000000  1.000000  0.995495  0.000000  0.25   \n",
              "55   0   0   4  221  1.00  1.000000  1.000000  1.000000  0.000000  0.00   \n",
              "\n",
              "         FDR       ACC  \n",
              "0   0.000000  1.000000  \n",
              "1   0.000000  1.000000  \n",
              "2   0.000000  1.000000  \n",
              "3   0.250000  0.991111  \n",
              "4   0.000000  1.000000  \n",
              "5   0.000000  1.000000  \n",
              "6   0.000000  1.000000  \n",
              "7   0.000000  1.000000  \n",
              "8   0.555556  0.977778  \n",
              "9   0.250000  0.991111  \n",
              "10  0.200000  0.995556  \n",
              "11  0.000000  1.000000  \n",
              "12  0.000000  1.000000  \n",
              "13  0.000000  0.995556  \n",
              "14  0.000000  1.000000  \n",
              "15  0.000000  1.000000  \n",
              "16  0.000000  1.000000  \n",
              "17  0.000000  1.000000  \n",
              "18  0.000000  0.991111  \n",
              "19  0.000000  0.995556  \n",
              "20  0.000000  1.000000  \n",
              "21  0.700000  0.964444  \n",
              "22       NaN  0.982222  \n",
              "23  0.200000  0.995556  \n",
              "24  0.000000  1.000000  \n",
              "25  0.666667  0.964444  \n",
              "26  0.000000  0.991111  \n",
              "27  0.000000  1.000000  \n",
              "28  0.000000  0.995556  \n",
              "29  0.000000  1.000000  \n",
              "30  0.000000  1.000000  \n",
              "31  0.333333  0.986667  \n",
              "32  0.000000  1.000000  \n",
              "33  0.000000  1.000000  \n",
              "34  0.000000  0.991111  \n",
              "35  0.000000  1.000000  \n",
              "36  0.000000  1.000000  \n",
              "37  0.000000  1.000000  \n",
              "38  0.000000  0.991111  \n",
              "39  0.000000  0.995556  \n",
              "40  0.666667  0.977778  \n",
              "41  0.000000  0.991111  \n",
              "42  0.000000  1.000000  \n",
              "43  0.000000  1.000000  \n",
              "44  0.000000  1.000000  \n",
              "45  0.000000  1.000000  \n",
              "46  0.000000  0.991111  \n",
              "47  0.000000  0.991111  \n",
              "48  0.600000  0.968889  \n",
              "49  0.333333  0.986667  \n",
              "50  0.000000  1.000000  \n",
              "51  0.250000  0.991111  \n",
              "52  0.250000  0.991111  \n",
              "53  0.000000  1.000000  \n",
              "54  0.000000  0.995556  \n",
              "55  0.000000  1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2d05b3df-a755-40c4-bf3a-998a9ebee723\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "      <th>TP</th>\n",
              "      <th>TN</th>\n",
              "      <th>TPR</th>\n",
              "      <th>TNR</th>\n",
              "      <th>PPV</th>\n",
              "      <th>NPV</th>\n",
              "      <th>FPR</th>\n",
              "      <th>FNR</th>\n",
              "      <th>FDR</th>\n",
              "      <th>ACC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>220</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.995475</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.995475</td>\n",
              "      <td>0.004525</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.991111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>216</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.977376</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.022624</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.977778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>220</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.995475</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.995475</td>\n",
              "      <td>0.004525</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.991111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>220</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.995475</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.004525</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.995556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>221</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995495</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.995556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>221</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.991031</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.991111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>221</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995495</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.995556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>214</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.968326</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.995349</td>\n",
              "      <td>0.031674</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.964444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.982222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.982222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>220</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.995475</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.004525</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.995556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>213</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.963801</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.036199</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.964444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>221</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.991031</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.991111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>221</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995495</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.995556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>220</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.995475</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.990991</td>\n",
              "      <td>0.004525</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.986667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>221</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.991031</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.991111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>221</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.991031</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.991111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>221</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995495</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.995556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>219</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.990950</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.986486</td>\n",
              "      <td>0.009050</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.977778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>221</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.991031</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.991111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>221</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.991031</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.991111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>221</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.991031</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.991111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>214</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.972727</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.995349</td>\n",
              "      <td>0.027273</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.968889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>220</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.995475</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.990991</td>\n",
              "      <td>0.004525</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.986667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>220</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.995475</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.995475</td>\n",
              "      <td>0.004525</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.991111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>220</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.995475</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.995475</td>\n",
              "      <td>0.004525</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.991111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>221</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995495</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.995556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2d05b3df-a755-40c4-bf3a-998a9ebee723')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2d05b3df-a755-40c4-bf3a-998a9ebee723 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2d05b3df-a755-40c4-bf3a-998a9ebee723');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"Metrics.csv\",index=True)\n",
        "df.to_csv(\"Metrics\",index=True)\n"
      ],
      "metadata": {
        "id": "pR0xsSh8bQGu"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cr = metrics.classification_report(Actual,Predicted, output_dict =True)\n",
        "cr = pd.DataFrame(cr).T\n",
        "cr.to_csv(\"Classification_Report.csv\", index = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO6c0e1tbYod",
        "outputId": "cf184051-5aa0-41eb-fde5-0855b447c65e"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(Actual,Predicted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eLhd5WJNQCD",
        "outputId": "d75901b6-2176-404f-f74c-6db380a34429"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           2       1.00      1.00      1.00         4\n",
            "           3       1.00      1.00      1.00         4\n",
            "           4       1.00      1.00      1.00         4\n",
            "           5       0.75      0.75      0.75         4\n",
            "           6       1.00      1.00      1.00         4\n",
            "           7       1.00      1.00      1.00         4\n",
            "           8       1.00      1.00      1.00         4\n",
            "           9       1.00      1.00      1.00         4\n",
            "          10       0.44      1.00      0.62         4\n",
            "          11       0.75      0.75      0.75         4\n",
            "          12       0.80      1.00      0.89         4\n",
            "          13       1.00      1.00      1.00         4\n",
            "          14       1.00      1.00      1.00         4\n",
            "          15       1.00      0.75      0.86         4\n",
            "          16       1.00      1.00      1.00         4\n",
            "          17       1.00      1.00      1.00         4\n",
            "          18       1.00      1.00      1.00         4\n",
            "          19       1.00      1.00      1.00         4\n",
            "          20       1.00      0.50      0.67         4\n",
            "          21       1.00      0.75      0.86         4\n",
            "          22       1.00      1.00      1.00         4\n",
            "          23       0.30      0.75      0.43         4\n",
            "          24       0.00      0.00      0.00         4\n",
            "          25       0.80      1.00      0.89         4\n",
            "          26       1.00      1.00      1.00         4\n",
            "          27       0.33      1.00      0.50         4\n",
            "          28       1.00      0.50      0.67         4\n",
            "          29       1.00      1.00      1.00         4\n",
            "          30       1.00      0.75      0.86         4\n",
            "          31       1.00      1.00      1.00         4\n",
            "          32       1.00      1.00      1.00         4\n",
            "          33       0.67      0.50      0.57         4\n",
            "          34       1.00      1.00      1.00         4\n",
            "          35       1.00      1.00      1.00         4\n",
            "          36       1.00      0.50      0.67         4\n",
            "          37       1.00      1.00      1.00         4\n",
            "          38       1.00      1.00      1.00         4\n",
            "          39       1.00      1.00      1.00         4\n",
            "          40       1.00      0.50      0.67         4\n",
            "          41       1.00      0.75      0.86         4\n",
            "          42       0.33      0.25      0.29         4\n",
            "          43       1.00      0.50      0.67         4\n",
            "          44       1.00      1.00      1.00         4\n",
            "          45       1.00      1.00      1.00         4\n",
            "          46       1.00      1.00      1.00         4\n",
            "          47       1.00      1.00      1.00         4\n",
            "          48       1.00      0.50      0.67         4\n",
            "          49       1.00      0.50      0.67         4\n",
            "          50       0.40      0.80      0.53         5\n",
            "          51       0.67      0.50      0.57         4\n",
            "          52       1.00      1.00      1.00         4\n",
            "          53       0.75      0.75      0.75         4\n",
            "          54       0.75      0.75      0.75         4\n",
            "          55       1.00      1.00      1.00         4\n",
            "          56       1.00      0.75      0.86         4\n",
            "          57       1.00      1.00      1.00         4\n",
            "\n",
            "    accuracy                           0.84       225\n",
            "   macro avg       0.89      0.84      0.84       225\n",
            "weighted avg       0.89      0.84      0.84       225\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Euclidean Distance"
      ],
      "metadata": {
        "id": "oz8JAWmMV_Vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TP=0\n",
        "TOTAL=0\n",
        "folder=0\n",
        "distance = float(\"inf\")\n",
        "for face_names in os.listdir(test_data):\n",
        "    person_dir = os.path.join(test_data,face_names)\n",
        "\n",
        "    for image_name in os.listdir(person_dir):\n",
        "        image_path = os.path.join(person_dir,image_name)\n",
        "\n",
        "        img_BGR = cv.imread(image_path)\n",
        "        img_RGB = cv.cvtColor(img_BGR, cv.COLOR_BGR2RGB)\n",
        "        \n",
        "        face = normalize(img_RGB)\n",
        "        face = cv.resize(face, required_shape, interpolation = cv.INTER_CUBIC)\n",
        "\n",
        "        face_d = np.expand_dims(face, axis=0)\n",
        "\n",
        "        test_encode = face_encoder.predict(face_d)[0]\n",
        "\n",
        "        for db_name, db_encode in encoding_dict.items():\n",
        "          dist = euclidean(db_encode, test_encode)\n",
        "          if dist < distance:\n",
        "              name = db_name\n",
        "              distance = dist\n",
        "        distance = float(\"inf\")\n",
        "        \n",
        "        if name == face_names:\n",
        "          TP+=1\n",
        "        else:\n",
        "          print(\"predicted = \",name,\" -> but expected =\", face_names)\n",
        "        TOTAL+=1\n",
        "        print(\"True Positives = \", TP,\"Total_images =\", TOTAL)\n",
        "    \n",
        "print(\"Accuracy = \", TP/TOTAL)"
      ],
      "metadata": {
        "id": "m6gzRwlgV-Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine Distance Testing"
      ],
      "metadata": {
        "id": "hV3J7YhNWElY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TP=0\n",
        "TOTAL=0\n",
        "folder=0\n",
        "distance = float(\"inf\")\n",
        "for face_names in os.listdir(test_data):\n",
        "    person_dir = os.path.join(test_data,face_names)\n",
        "\n",
        "    for image_name in os.listdir(person_dir):\n",
        "        image_path = os.path.join(person_dir,image_name)\n",
        "\n",
        "        img_BGR = cv.imread(image_path)\n",
        "        img_RGB = cv.cvtColor(img_BGR, cv.COLOR_BGR2RGB)\n",
        "        \n",
        "        face = normalize(img_RGB)\n",
        "        face = cv.resize(face, required_shape, interpolation = cv.INTER_CUBIC)\n",
        "\n",
        "        face_d = np.expand_dims(face, axis=0)\n",
        "\n",
        "        test_encode = face_encoder.predict(face_d)[0]\n",
        "\n",
        "        for db_name, db_encode in encoding_dict.items():\n",
        "          dist = cosine(db_encode, test_encode)\n",
        "          if dist < distance:\n",
        "              name = db_name\n",
        "              distance = dist\n",
        "        distance = float(\"inf\")\n",
        "        \n",
        "        if name == face_names:\n",
        "          TP+=1\n",
        "        else:\n",
        "          print(\"predicted = \",name,\" -> but expected =\", face_names)\n",
        "        TOTAL+=1\n",
        "        print(\"True Positives = \", TP,\"Total_images =\", TOTAL)\n",
        "    \n",
        "print(\"Accuracy = \", TP/TOTAL)"
      ],
      "metadata": {
        "id": "JF1AaxjUWHOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Squared Euclidean distance Testing"
      ],
      "metadata": {
        "id": "3KKCtSU5WLpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TP=0\n",
        "TOTAL=0\n",
        "folder=0\n",
        "for face_names in os.listdir(test_data):\n",
        "    person_dir = os.path.join(test_data,face_names)\n",
        "\n",
        "    for image_name in os.listdir(person_dir):\n",
        "        image_path = os.path.join(person_dir,image_name)\n",
        "\n",
        "        img_BGR = cv.imread(image_path)\n",
        "        img_RGB = cv.cvtColor(img_BGR, cv.COLOR_BGR2RGB)\n",
        "        \n",
        "        face = normalize(img_RGB)\n",
        "        face = cv.resize(face, required_shape, interpolation = cv.INTER_CUBIC)\n",
        "\n",
        "        face_d = np.expand_dims(face, axis=0)\n",
        "\n",
        "        test_encode = face_encoder.predict(face_d)[0]\n",
        "\n",
        "        for db_name, db_encode in encoding_dict.items():\n",
        "          dist = sqeuclidean(db_encode, test_encode)\n",
        "          if dist < distance:\n",
        "              name = db_name\n",
        "              distance = dist\n",
        "              print(\"BEFORE distance = \", dist , \"and db_name = \", db_name, \"and name = \", name, \"and face name = \", face_names)\n",
        "        print(\"AFTER distance = \", dist , \"and db_name = \", db_name, \"and name = \", name, \"and face name = \", face_names)\n",
        "        distance = float(\"inf\")\n",
        "        \n",
        "        if name == face_names:\n",
        "          TP+=1\n",
        "        TOTAL+=1\n",
        "        print(\"True Positives = \", TP,\"Total_images =\", TOTAL)\n",
        "    \n",
        "print(\"Accuracy = \", TP/TOTAL)\n",
        "    # folder+=1\n",
        "    # if folder == 10:\n",
        "    #   break"
      ],
      "metadata": {
        "id": "3KnullU3WNwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Research Questions:"
      ],
      "metadata": {
        "id": "yPl7B6DMWcz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(research)\n",
        "print(len(research))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au5RdIB4Wetj",
        "outputId": "df679971-a833-4647-e6a0-9603e74cba61"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['28', 'thiago_medeiros-7-30.png'], ['28', 'thiago_medeiros-25-26.png'], ['21', 'NikhilKumar_Jalagam-2-16.png'], ['23', 'Srikarrao_Boinapally-25-30.png'], ['24', 'aidan_jamai-2-14.png'], ['24', 'aidan_jamai-7-20.png'], ['24', 'aidan_jamai-25-03.png'], ['24', 'aidan_jamai-4-14.png'], ['20', 'osamabin-basit-2-18.png'], ['20', 'osamabin-basit-25-19.png'], ['15', 'Sumit Jadhav-19-16.png'], ['11', 'sanskar_bhavnani-4-12.png'], ['05', 'caitlin_hays-7-14.png'], ['50', 'rishith_thipireddy-26-1.png'], ['54', 'sreevardhan_ginjupalli-7-21.png'], ['53', 'Suraj_Kumar-2-1.png'], ['49', 'photos 3abhiram_anand-2-5.png'], ['49', 'photos 3abhiram_anand-7-25.png'], ['48', 'project_imagescourtney_ruble-7-21.png'], ['48', 'project_imagescourtney_ruble-25-25.png'], ['51', 'Adams_Samuel-2-16.png'], ['51', 'Adams_Samuel-7-19.png'], ['56', 'processedyarehalli_prerna-5-15.png'], ['42', 'amal_guduru-2-12.png'], ['42', 'amal_guduru-25-25.png'], ['42', 'amal_guduru-4-13.png'], ['41', 'mahanthsai_kilaru-2-10.png'], ['43', 'processed_imagesbrandon_buckley-2-14.png'], ['43', 'processed_imagesbrandon_buckley-7-17.png'], ['40', 'ozge_tutar-2.png'], ['40', 'ozge_tutar-4.png'], ['36', 'Inioluwa_Adegoke-2-14.png'], ['36', 'Inioluwa_Adegoke-7-17.png'], ['30', 'Pictures_new1Lokesh_Guduru-4-5.png'], ['33', 'Mukund_Sharma-25-26.png'], ['33', 'Mukund_Sharma-4-16.png']]\n",
            "36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of 36 wrong predictions, only 4 correspond to female, while 32 correspond to male."
      ],
      "metadata": {
        "id": "c4LipWNmZMzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    output_data + '/val',\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",\n",
        "    class_names=None,\n",
        "    color_mode=\"grayscale\",\n",
        "    batch_size=1,\n",
        "    image_size=(100, 100),\n",
        "    shuffle=True,\n",
        "    seed=1234,\n",
        "    validation_split=None,\n",
        "    subset=None,\n",
        "    interpolation=\"bilinear\",\n",
        "    follow_links=False,\n",
        "    crop_to_aspect_ratio=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C46XxSsKYnyw",
        "outputId": "ae38fd58-6dee-4e8c-93c5-76e4f19f6ca6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 225 files belonging to 56 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for data,label in training_data:\n",
        "  print(data.shape, label)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik3q1t_NZ2vx",
        "outputId": "fe15140a-916b-4b74-f2e9-a64815e53b95"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 100, 100, 1) tf.Tensor([52], shape=(1,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TP=0\n",
        "TOTAL=0\n",
        "folder=0\n",
        "distance = float(\"inf\")\n",
        "research = []\n",
        "for face_names in os.listdir(test_data):\n",
        "    person_dir = os.path.join(test_data,face_names)\n",
        "\n",
        "    for image_name in os.listdir(person_dir):\n",
        "        image_path = os.path.join(person_dir,image_name)\n",
        "\n",
        "        img_BGR = cv.imread(image_path)\n",
        "        img_RGB = cv.cvtColor(img_BGR, cv.COLOR_BGR2GRAY)\n",
        "        \n",
        "        face = normalize(img_RGB)\n",
        "        face = cv.resize(face, required_shape, interpolation = cv.INTER_CUBIC)\n",
        "\n",
        "        face_d = np.expand_dims(face, axis=0)\n",
        "        face_d = tf.stack([face_d,face_d,face_d], axis = -1)\n",
        "        print(face_d.shape)\n",
        "        test_encode = face_encoder.predict(face_d)[0]\n",
        "\n",
        "        for db_name, db_encode in encoding_dict.items():\n",
        "          dist = cosine(db_encode, test_encode)\n",
        "          dist_0 = euclidean(db_encode, test_encode)\n",
        "          dist_f = 0.3 * dist_0 + 0.7 * dist\n",
        "          \n",
        "          if dist < recognition_t and dist < distance:\n",
        "              name = db_name\n",
        "              distance = dist\n",
        "        distance = float(\"inf\")\n",
        "        \n",
        "        if name == face_names:\n",
        "          TP+=1\n",
        "        else:\n",
        "          print(\"predicted = \",name,\" -> but expected =\", face_names)\n",
        "          research.append([face_names, image_name])\n",
        "        TOTAL+=1\n",
        "        print(\"True Positives = \", TP,\"Total_images =\", TOTAL)\n",
        "    \n",
        "print(\"Accuracy = \", TP/TOTAL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RH_tiJBzZ78E",
        "outputId": "08b287ea-61bd-41fa-b2b8-5003e8ed093b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "True Positives =  1 Total_images = 1\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "True Positives =  2 Total_images = 2\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "True Positives =  3 Total_images = 3\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  4 Total_images = 4\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  5 Total_images = 5\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "predicted =  27  -> but expected = 28\n",
            "True Positives =  5 Total_images = 6\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "predicted =  50  -> but expected = 28\n",
            "True Positives =  5 Total_images = 7\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  6 Total_images = 8\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  7 Total_images = 9\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  8 Total_images = 10\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  9 Total_images = 11\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  10 Total_images = 12\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  11 Total_images = 13\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  12 Total_images = 14\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  13 Total_images = 15\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "True Positives =  14 Total_images = 16\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "predicted =  11  -> but expected = 21\n",
            "True Positives =  14 Total_images = 17\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  15 Total_images = 18\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  16 Total_images = 19\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  17 Total_images = 20\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  18 Total_images = 21\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "True Positives =  19 Total_images = 22\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  20 Total_images = 23\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "True Positives =  21 Total_images = 24\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  22 Total_images = 25\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  23 Total_images = 26\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  24 Total_images = 27\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  25 Total_images = 28\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  26 Total_images = 29\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  27 Total_images = 30\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  28 Total_images = 31\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "True Positives =  29 Total_images = 32\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  30 Total_images = 33\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  31 Total_images = 34\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  32 Total_images = 35\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  33 Total_images = 36\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "predicted =  23  -> but expected = 24\n",
            "True Positives =  33 Total_images = 37\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "predicted =  23  -> but expected = 24\n",
            "True Positives =  33 Total_images = 38\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "predicted =  23  -> but expected = 24\n",
            "True Positives =  33 Total_images = 39\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "predicted =  23  -> but expected = 24\n",
            "True Positives =  33 Total_images = 40\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "predicted =  23  -> but expected = 20\n",
            "True Positives =  33 Total_images = 41\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  34 Total_images = 42\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "predicted =  53  -> but expected = 20\n",
            "True Positives =  34 Total_images = 43\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "predicted =  53  -> but expected = 20\n",
            "True Positives =  34 Total_images = 44\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  35 Total_images = 45\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  36 Total_images = 46\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "True Positives =  37 Total_images = 47\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  38 Total_images = 48\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  39 Total_images = 49\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  40 Total_images = 50\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  41 Total_images = 51\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  42 Total_images = 52\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "True Positives =  43 Total_images = 53\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "True Positives =  44 Total_images = 54\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "predicted =  51  -> but expected = 10\n",
            "True Positives =  44 Total_images = 55\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  45 Total_images = 56\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  46 Total_images = 57\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  47 Total_images = 58\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  48 Total_images = 59\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "True Positives =  49 Total_images = 60\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  50 Total_images = 61\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  51 Total_images = 62\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  52 Total_images = 63\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  53 Total_images = 64\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  54 Total_images = 65\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  55 Total_images = 66\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  56 Total_images = 67\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  57 Total_images = 68\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  58 Total_images = 69\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  59 Total_images = 70\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  60 Total_images = 71\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  61 Total_images = 72\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  62 Total_images = 73\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  63 Total_images = 74\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  64 Total_images = 75\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "True Positives =  65 Total_images = 76\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "predicted =  23  -> but expected = 15\n",
            "True Positives =  65 Total_images = 77\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "True Positives =  66 Total_images = 78\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  67 Total_images = 79\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  68 Total_images = 80\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  69 Total_images = 81\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "True Positives =  70 Total_images = 82\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  71 Total_images = 83\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "True Positives =  72 Total_images = 84\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  73 Total_images = 85\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  74 Total_images = 86\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "True Positives =  75 Total_images = 87\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "predicted =  33  -> but expected = 11\n",
            "True Positives =  75 Total_images = 88\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  76 Total_images = 89\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  77 Total_images = 90\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  78 Total_images = 91\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  79 Total_images = 92\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  80 Total_images = 93\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "True Positives =  81 Total_images = 94\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "True Positives =  82 Total_images = 95\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  83 Total_images = 96\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  84 Total_images = 97\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  85 Total_images = 98\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "True Positives =  86 Total_images = 99\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  87 Total_images = 100\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  88 Total_images = 101\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  89 Total_images = 102\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  90 Total_images = 103\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "True Positives =  91 Total_images = 104\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  92 Total_images = 105\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  93 Total_images = 106\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  94 Total_images = 107\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "True Positives =  95 Total_images = 108\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  96 Total_images = 109\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "True Positives =  97 Total_images = 110\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  98 Total_images = 111\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "True Positives =  99 Total_images = 112\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  100 Total_images = 113\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "predicted =  50  -> but expected = 05\n",
            "True Positives =  100 Total_images = 114\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  101 Total_images = 115\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  102 Total_images = 116\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  103 Total_images = 117\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "True Positives =  104 Total_images = 118\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "True Positives =  105 Total_images = 119\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "True Positives =  106 Total_images = 120\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  107 Total_images = 121\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  108 Total_images = 122\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "predicted =  42  -> but expected = 54\n",
            "True Positives =  108 Total_images = 123\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "True Positives =  109 Total_images = 124\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  110 Total_images = 125\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "predicted =  42  -> but expected = 53\n",
            "True Positives =  110 Total_images = 126\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  111 Total_images = 127\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "predicted =  23  -> but expected = 53\n",
            "True Positives =  111 Total_images = 128\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "True Positives =  112 Total_images = 129\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "predicted =  10  -> but expected = 49\n",
            "True Positives =  112 Total_images = 130\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "predicted =  50  -> but expected = 49\n",
            "True Positives =  112 Total_images = 131\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  113 Total_images = 132\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "True Positives =  114 Total_images = 133\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  115 Total_images = 134\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "predicted =  27  -> but expected = 48\n",
            "True Positives =  115 Total_images = 135\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "predicted =  50  -> but expected = 48\n",
            "True Positives =  115 Total_images = 136\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "True Positives =  116 Total_images = 137\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  117 Total_images = 138\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "predicted =  27  -> but expected = 51\n",
            "True Positives =  117 Total_images = 139\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "True Positives =  118 Total_images = 140\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  119 Total_images = 141\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  120 Total_images = 142\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "predicted =  17  -> but expected = 57\n",
            "True Positives =  120 Total_images = 143\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "True Positives =  121 Total_images = 144\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "True Positives =  122 Total_images = 145\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  123 Total_images = 146\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  124 Total_images = 147\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  125 Total_images = 148\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  126 Total_images = 149\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  127 Total_images = 150\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "True Positives =  128 Total_images = 151\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  129 Total_images = 152\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "predicted =  12  -> but expected = 56\n",
            "True Positives =  129 Total_images = 153\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  130 Total_images = 154\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  131 Total_images = 155\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "True Positives =  132 Total_images = 156\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "predicted =  28  -> but expected = 52\n",
            "True Positives =  132 Total_images = 157\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "predicted =  50  -> but expected = 42\n",
            "True Positives =  132 Total_images = 158\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  133 Total_images = 159\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "predicted =  25  -> but expected = 42\n",
            "True Positives =  133 Total_images = 160\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "predicted =  50  -> but expected = 42\n",
            "True Positives =  133 Total_images = 161\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  134 Total_images = 162\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  135 Total_images = 163\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  136 Total_images = 164\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  137 Total_images = 165\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  138 Total_images = 166\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  139 Total_images = 167\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  140 Total_images = 168\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  141 Total_images = 169\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  142 Total_images = 170\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  143 Total_images = 171\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "True Positives =  144 Total_images = 172\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "predicted =  54  -> but expected = 44\n",
            "True Positives =  144 Total_images = 173\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "predicted =  10  -> but expected = 41\n",
            "True Positives =  144 Total_images = 174\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "True Positives =  145 Total_images = 175\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  146 Total_images = 176\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  147 Total_images = 177\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "True Positives =  148 Total_images = 178\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "True Positives =  149 Total_images = 179\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  150 Total_images = 180\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  151 Total_images = 181\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "predicted =  50  -> but expected = 43\n",
            "True Positives =  151 Total_images = 182\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "predicted =  27  -> but expected = 43\n",
            "True Positives =  151 Total_images = 183\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  152 Total_images = 184\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "True Positives =  153 Total_images = 185\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  154 Total_images = 186\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "True Positives =  155 Total_images = 187\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "predicted =  23  -> but expected = 46\n",
            "True Positives =  155 Total_images = 188\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  156 Total_images = 189\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  157 Total_images = 190\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "predicted =  24  -> but expected = 39\n",
            "True Positives =  157 Total_images = 191\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  158 Total_images = 192\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  159 Total_images = 193\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "predicted =  23  -> but expected = 40\n",
            "True Positives =  159 Total_images = 194\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  160 Total_images = 195\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  161 Total_images = 196\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "predicted =  28  -> but expected = 40\n",
            "True Positives =  161 Total_images = 197\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  162 Total_images = 198\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "True Positives =  163 Total_images = 199\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "True Positives =  164 Total_images = 200\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "predicted =  39  -> but expected = 31\n",
            "True Positives =  164 Total_images = 201\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  165 Total_images = 202\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  166 Total_images = 203\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  167 Total_images = 204\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  168 Total_images = 205\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "predicted =  50  -> but expected = 36\n",
            "True Positives =  168 Total_images = 206\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "predicted =  50  -> but expected = 36\n",
            "True Positives =  168 Total_images = 207\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  169 Total_images = 208\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "True Positives =  170 Total_images = 209\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  171 Total_images = 210\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  172 Total_images = 211\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "True Positives =  173 Total_images = 212\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "predicted =  10  -> but expected = 30\n",
            "True Positives =  173 Total_images = 213\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  174 Total_images = 214\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  175 Total_images = 215\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "True Positives =  176 Total_images = 216\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "True Positives =  177 Total_images = 217\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "True Positives =  178 Total_images = 218\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "True Positives =  179 Total_images = 219\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "True Positives =  180 Total_images = 220\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "predicted =  23  -> but expected = 33\n",
            "True Positives =  180 Total_images = 221\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "True Positives =  181 Total_images = 222\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "True Positives =  182 Total_images = 223\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "True Positives =  183 Total_images = 224\n",
            "(1, 100, 100, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "True Positives =  184 Total_images = 225\n",
            "Accuracy =  0.8177777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QkY6-SpVaTnW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}